# OllamaTalk

## [Overview](pplx://action/followup)

OllamaTalk is a fully **local, cross-platform AI chat application** that runs seamlessly on macOS,
Windows, Linux, Android, and iOS. All AI processing happens entirely **on your device**, ensuring a
secure and private chat experience without relying on external servers or cloud services. This
design guarantees complete control over your data while delivering a unified experience across all
major platforms.

## [Cross-Platform Support](pplx://action/followup)

- **[Desktop Platforms](pplx://action/followup)**
    - macOS 11 Big Sur or later
    - Windows 10/11
    - Linux (Ubuntu 20.04 or later recommended)
- **[Mobile Platforms](pplx://action/followup)**
    - Android 6.0 (Marshmallow) or later
    - iOS (Supports `.ipa` sideloading)

## [Installation](pplx://action/followup)

### [Step 1: Install Ollama Server](pplx://action/followup)

- Download and install Ollama from the [official download page](https://ollama.com/download).

### [Step 2: Download AI Models](pplx://action/followup)

- Browse and download your preferred models from
  the [Ollama Model Hub](https://ollama.com/search).  
  Examples: deepseek-r1, llama, mistral, qwen, gemma2, llava, and more.

### [Step 3: Start Ollama Server](pplx://action/followup)

Run the following command to start the Ollama server:

```
OLLAMA_HOST=0.0.0.0:11434 ollama serve
```

### [Step 4: Run the Application](pplx://action/followup)

1. Visit the [OllamaTalk Releases](https://github.com/shinhyo/OllamaTalk/releases) page.
2. Download the latest version for your platform

### [Step 5: Launch OllamaTalk](pplx://action/followup)

1. Open the installed application.
2. Connect to your local Ollama server.
3. Start chatting with AI.

> **Note:** Ensure that the Ollama server is running before launching the application.

## [License](pplx://action/followup)

This project is licensed under the MIT License.

## [Support](pplx://action/followup)

- Report bugs and suggest features through GitHub Issues.
- Contributions via Pull Requests are welcome!
